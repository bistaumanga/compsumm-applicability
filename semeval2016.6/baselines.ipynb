{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import readline\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "tt = TweetTokenizer(preserve_case=False)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.update({\"#semst\", \"...\", \"rt\"})\n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def tokenize(t):\n",
    "    tokens = tt.tokenize(t)\n",
    "    tokens = [tok for tok in tokens if not ((len(tok) == 1 and not tok.isalpha()) or tok in stops)]\n",
    "    tokens = [ (ps.stem(tok) if tok[0] != \"#\" else tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tedcruz And, #HandOverTheServer she wiped cle...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[@tedcruz, #handovertheserver, wipe, clean, 30...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>[hillari, best, choic, truli, want, continu, p...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TheView I think our country is ready for a fe...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[@theview, think, countri, readi, femal, pre, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[gave, unhealthi, amount, hard-earn, money, aw...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PortiaABoulger Thank you for adding me to you...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>NONE</td>\n",
       "      <td>[@portiaaboulg, thank, ad, list]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet           Target  \\\n",
       "0  @tedcruz And, #HandOverTheServer she wiped cle...  Hillary Clinton   \n",
       "1  Hillary is our best choice if we truly want to...  Hillary Clinton   \n",
       "2  @TheView I think our country is ready for a fe...  Hillary Clinton   \n",
       "3  I just gave an unhealthy amount of my hard-ear...  Hillary Clinton   \n",
       "4  @PortiaABoulger Thank you for adding me to you...  Hillary Clinton   \n",
       "\n",
       "    Stance                                             tokens  num_tokens  \n",
       "0  AGAINST  [@tedcruz, #handovertheserver, wipe, clean, 30...          14  \n",
       "1    FAVOR  [hillari, best, choic, truli, want, continu, p...           9  \n",
       "2  AGAINST  [@theview, think, countri, readi, femal, pre, ...           9  \n",
       "3  AGAINST  [gave, unhealthi, amount, hard-earn, money, aw...          11  \n",
       "4     NONE                   [@portiaaboulg, thank, ad, list]           4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../semeval2016.6/StanceDataset/train.csv\", encoding= 'unicode_escape', engine='python')\n",
    "del train[\"Opinion Towards\"]\n",
    "del train[\"Sentiment\"]\n",
    "train[\"tokens\"] = [ tokenize(t) for t in train[\"Tweet\"].values ]\n",
    "train[\"num_tokens\"] = [len(tt) for tt in train[\"tokens\"].values ]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He who exalts himself shall      be humbled; a...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[exalt, shall, humbl, humbl, shall, exalted.ma...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[@prayerbullet, remov, nehushtan, previou, mov...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[@brainman365, @heidtjj, @benjaminl, sought, t...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#God is utterly powerless without Human interv...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[#god, utterli, powerless, without, human, int...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>[@david_cameron, miracl, #multiculturalism, mi...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet   Target   Stance  \\\n",
       "0  He who exalts himself shall      be humbled; a...  Atheism  AGAINST   \n",
       "1  RT @prayerbullets: I remove Nehushtan -previou...  Atheism  AGAINST   \n",
       "2  @Brainman365 @heidtjj @BenjaminLives I have so...  Atheism  AGAINST   \n",
       "3  #God is utterly powerless without Human interv...  Atheism  AGAINST   \n",
       "4  @David_Cameron   Miracles of #Multiculturalism...  Atheism  AGAINST   \n",
       "\n",
       "                                              tokens  num_tokens  \n",
       "0  [exalt, shall, humbl, humbl, shall, exalted.ma...           7  \n",
       "1  [@prayerbullet, remov, nehushtan, previou, mov...          12  \n",
       "2  [@brainman365, @heidtjj, @benjaminl, sought, t...          11  \n",
       "3  [#god, utterli, powerless, without, human, int...           6  \n",
       "4  [@david_cameron, miracl, #multiculturalism, mi...          13  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../semeval2016.6/StanceDataset/test.csv\", encoding= 'unicode_escape', engine='python')\n",
    "del test[\"Opinion Towards\"]\n",
    "del test[\"Sentiment\"]\n",
    "test[\"tokens\"] = [ tokenize(t) for t in test[\"Tweet\"].values ]\n",
    "test[\"num_tokens\"] = [len(tt) for tt in test[\"tokens\"].values ]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Stance</th>\n",
       "      <th>AGAINST</th>\n",
       "      <th>FAVOR</th>\n",
       "      <th>NONE</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atheism</th>\n",
       "      <td>304</td>\n",
       "      <td>92</td>\n",
       "      <td>117</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Climate Change is a Real Concern</th>\n",
       "      <td>15</td>\n",
       "      <td>212</td>\n",
       "      <td>168</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feminist Movement</th>\n",
       "      <td>328</td>\n",
       "      <td>210</td>\n",
       "      <td>126</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillary Clinton</th>\n",
       "      <td>393</td>\n",
       "      <td>118</td>\n",
       "      <td>178</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Legalization of Abortion</th>\n",
       "      <td>355</td>\n",
       "      <td>121</td>\n",
       "      <td>177</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1395</td>\n",
       "      <td>753</td>\n",
       "      <td>766</td>\n",
       "      <td>2914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stance                            AGAINST  FAVOR  NONE   All\n",
       "Target                                                      \n",
       "Atheism                               304     92   117   513\n",
       "Climate Change is a Real Concern       15    212   168   395\n",
       "Feminist Movement                     328    210   126   664\n",
       "Hillary Clinton                       393    118   178   689\n",
       "Legalization of Abortion              355    121   177   653\n",
       "All                                  1395    753   766  2914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=train['Target'], columns=train['Stance'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(target, keyword):\n",
    "    subset = train.query('Target==@target')\n",
    "    filtered = subset[subset.apply(lambda row: keyword in row[\"tokens\"], axis=1)]\n",
    "    return filtered[[\"Tweet\", \"Stance\"]]#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('god', 101),\n",
       " ('#god', 50),\n",
       " ('love', 35),\n",
       " ('us', 34),\n",
       " ('religion', 33),\n",
       " ('peopl', 31),\n",
       " ('faith', 29),\n",
       " ('believ', 26),\n",
       " ('jesu', 26),\n",
       " ('lord', 25),\n",
       " ('day', 25),\n",
       " ('one', 25),\n",
       " ('go', 25),\n",
       " ('life', 24),\n",
       " ('make', 23),\n",
       " ('know', 23),\n",
       " ('world', 20),\n",
       " ('#islam', 20),\n",
       " ('#lovewins', 18),\n",
       " ('like', 17)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain, product\n",
    "all_tokens = Counter(chain(*train.query('Target==\"Atheism\"')[\"tokens\"]))\n",
    "# plt.hist(np.log2(list(all_tokens.values()) ), log=True)\n",
    "all_tokens.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "# AGAINST: jesu, lord, bless | FAVOR: #freethinker\n",
    "# AGAINST #gamergate #spankafeminist \n",
    "# AGAINST #prolifeyouth, kill, murder, life\n",
    "# AGAINST #wakeupamerica email #benghazi | @hillaryclinton @barackobama support\n",
    "# FAVOR #environment\n",
    "# print(search(\"Hillary Clinton\", \"right\").to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(target, pos_kws, neg_kws):\n",
    "    subset = train.query('Target==@target')\n",
    "    pos_idxs = subset.apply(lambda row: len (set(pos_kws) & set(row[\"tokens\"])) > 0, axis=1).values\n",
    "    neg_idxs = subset.apply(lambda row: len (set(neg_kws) & set(row[\"tokens\"])) > 0, axis=1).values\n",
    "    inferred = -1.0 * np.ones(len(subset))\n",
    "    inferred[pos_idxs] = 1.0\n",
    "    inferred[neg_idxs] = 0.0\n",
    "    subset[\"labels\"] = inferred\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submodular.models2 import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['AGAINST', 'FAVOR']\n",
    "\n",
    "HYPERPARAMS_GRID = {\n",
    "    \"lambdaa\": [0.1, 0.2, 0.3, 0.4],\n",
    "    \"C\": [0.1, 1.0, 10.0, 100.0],\n",
    "    \"gamma\": [0.0625, 0.125, 0.25, 0.5, 1.0, 2.0],\n",
    "    \"alpha\": [0.8, 0.85, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commons.functions import avg_f1 as scorer_func\n",
    "from commons.functions import each_f1\n",
    "mks = [5]\n",
    "CV = 3\n",
    "N_JOBS = 6 #int(sys.argv[1])\n",
    "scorer = make_scorer(scorer_func)\n",
    "STEPS = [\n",
    "    # [\"tok\", \"idf-bow\", \"cos\", \"greedy-diff\"],\n",
    "    [\"tok\", \"idf-bow\", \"exp\", \"greedy-diff\"],\n",
    "    # [\"tok\", \"idf-bow\", \"cos\", \"greedy-div\"],\n",
    "    [\"tok\", \"idf-bow\", \"exp\", \"greedy-div\"],\n",
    "    # [\"tok\", \"bow\", \"cos\", \"greedy-diff\"],\n",
    "    [\"tok\", \"bow\", \"exp\", \"greedy-diff\"],\n",
    "    # [\"tok\", \"bow\", \"cos\", \"greedy-div\"],\n",
    "    [\"tok\", \"bow\", \"exp\", \"greedy-div\"],\n",
    "    [\"tok\", \"idf-bow\", \"length\"],\n",
    "    # [\"tok\", \"idf-bow\", \"cos\", \"random\"],\n",
    "    [\"tok\", \"idf-bow\", \"cos\", \"pr\"],\n",
    "    [\"tok\", \"bow\", \"cos\", \"pr\"],\n",
    "    [\"tok\", \"idf-bow\", \"cos\", \"nn\"],\n",
    "    [\"tok\", \"bow\", \"cos\", \"nn\"],\n",
    "    [\"tok\", \"bow\", \"cos\", \"all\"],\n",
    "    [\"tok\", \"idf-bow\", \"cos\", \"all\"],\n",
    "    [\"tok\", \"bow\", \"exp\", \"all\"],\n",
    "    [\"tok\", \"idf-bow\", \"exp\", \"all\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Stance</th>\n",
       "      <th>AGAINST</th>\n",
       "      <th>FAVOR</th>\n",
       "      <th>NONE</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>325</td>\n",
       "      <td>97</td>\n",
       "      <td>160</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>393</td>\n",
       "      <td>118</td>\n",
       "      <td>178</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stance  AGAINST  FAVOR  NONE  All\n",
       "labels                           \n",
       "-1.0        325     97   160  582\n",
       "0.0          65      1     7   73\n",
       "1.0           3     20    11   34\n",
       "All         393    118   178  689"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target, pos, neg = \"Hillary Clinton\", [\"#lovewins\", \"@barackobama\", \"women\", \"proud\"], [\"#wakeupamerica\", \"email\", \"#benghazi\"]\n",
    "# target, pos, neg = \"Atheism\", [\"#freethinker\", \"#freedom\", \"religi\"], [\"jesu\", \"lord\", \"bless\", \"#teamjesus\"]\n",
    "subset_train = assign(target, pos, neg)\n",
    "pd.crosstab(index=subset_train['labels'], columns=subset_train['Stance'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 66, 1: 23}) Counter({-1: 172, 1: 45})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:17 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=0.1,lambda=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-diff', MMDGreedy(C=0.1, lambdaa=0.1, mk=5))])\n",
      "Hillary Clinton,tok_idf-bow_exp_greedy-diff,5,0.8495,0.543,8.8,8.6,0.7157,0.3704,0.543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:18 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=0.1,lambda=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-div',\n",
      "                 MMDGreedy(C=0.1, diff='div', lambdaa=0.1, mk=5))])\n",
      "Hillary Clinton,tok_idf-bow_exp_greedy-div,5,0.8384,0.5597,8.8,8.4,0.7829,0.3364,0.5597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:19 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=1.0,lambda=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-diff', MMDGreedy(C=1.0, lambdaa=0.1, mk=5))])\n",
      "Hillary Clinton,tok_bow_exp_greedy-diff,5,0.8522,0.5823,8.6,8.2,0.7613,0.4032,0.5823\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:20 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=1.0,lambda=0.1,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:20 INFO [LengthComp:275]=> fitted LengthComp(C=10.0,gamma=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-div',\n",
      "                 MMDGreedy(C=1.0, diff='div', lambdaa=0.1, mk=5))])\n",
      "Hillary Clinton,tok_bow_exp_greedy-div,5,0.9028,0.5823,8.6,8.2,0.7613,0.4032,0.5823\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')),\n",
      "                ('length', LengthComp(C=10.0, mk=5))])\n",
      "Hillary Clinton,tok_idf-bow_length,5,0.7218,0.4528,14,13,0.536,0.3696,0.4528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [PRComp:275]=> fitted PRComp(C=0.1,alpha=0.8,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('pr', PRComp(C=0.1, alpha=0.8, mk=5))])\n",
      "Hillary Clinton,tok_idf-bow_cos_pr,5,nan,0.2121,11,8.8,0.07778,0.3465,0.2121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [PRComp:275]=> fitted PRComp(C=0.1,alpha=0.8,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [NNComp:275]=> fitted NNComp(C=1.0,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [NNComp:275]=> fitted NNComp(C=1.0,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('pr', PRComp(C=0.1, alpha=0.8, mk=5))])\n",
      "Hillary Clinton,tok_bow_cos_pr,5,nan,0.1909,11,8.4,0.03429,0.3475,0.1909\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('nn', NNComp(C=1.0, mk=5))])\n",
      "Hillary Clinton,tok_idf-bow_cos_nn,5,0.7993,0.575,10,7.8,0.7297,0.4203,0.575\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('nn', NNComp(C=1.0, mk=5))])\n",
      "Hillary Clinton,tok_bow_cos_nn,5,0.7371,0.5568,7.8,8.8,0.6691,0.4444,0.5568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('all', AllComp(C=0.1))])\n",
      "Hillary Clinton,tok_bow_cos_all,5,0.5,0.5347,12,9.9,0.8842,0.1852,0.5347\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('all', AllComp(C=0.1))])\n",
      "Hillary Clinton,tok_idf-bow_cos_all,5,0.5,0.465,12,9.9,0.8866,0.04348,0.465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:21 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)), ('all', AllComp(C=0.1))])\n",
      "Hillary Clinton,tok_bow_exp_all,5,0.5,0.6074,12,9.9,0.7938,0.4211,0.6074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:19:22 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('all', AllComp(C=0.1))])\n",
      "Hillary Clinton,tok_idf-bow_exp_all,5,0.5,0.5538,12,9.9,0.854,0.2535,0.5538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_test = test.query('Target==@target')\n",
    "test_target = test.query('Target == @target & Stance == @lst').reset_index()\n",
    "train_target = subset_train.query('Target == @target & Stance == @lst & labels != -1').reset_index()\n",
    "\n",
    "y_tr = np.array( 2 * train_target.labels - 1, dtype = int)\n",
    "y_te = np.ones(len(test_target), dtype = np.int8)\n",
    "\n",
    "# y_tr[np.where(train_target.Stance.values == \"AGAINST\")] = -1\n",
    "y_te[np.where(test_target.Stance.values == \"AGAINST\")] = -1\n",
    "train_target[\"y\"] = y_tr\n",
    "test_target[\"y\"] = y_te\n",
    "    \n",
    "print(Counter(y_tr), Counter(y_te))\n",
    "\n",
    "\n",
    "for steps, mk in product(STEPS, mks):\n",
    "    pipe, _hyperparams = get_model(steps, mk = mk)\n",
    "    name = \"_\".join(steps)\n",
    "    hyperparams = {}\n",
    "    for hp in _hyperparams:\n",
    "        hp_name, step = hp\n",
    "        hyperparams[\"{}__{}\".format(step, hp_name)] = HYPERPARAMS_GRID[hp_name]\n",
    "        # if step == \"greedy-div\" and hp_name == \"lambdaa\":\n",
    "        #     hyperparams[\"{}__{}\".format(step, hp_name)] = HYPERPARAMS_GRID[\"lambda2\"]\n",
    "#     print(\"{}, m={}, params={}\".format(name, mk, \";\".join(hyperparams.keys()) ))\n",
    "\n",
    "    clf = GridSearchCV(Pipeline(pipe), hyperparams, \n",
    "                cv = CV, n_jobs = N_JOBS, pre_dispatch = N_JOBS, verbose = 0, scoring = scorer, \n",
    "                return_train_score = False, error_score = 0.5)\n",
    "    clf.fit(train_target, train_target.y.values)\n",
    "    train_score, test_score = clf.score(train_target, train_target.y.values), clf.score(test_target, test_target.y.values)\n",
    "    val_score = clf.best_score_\n",
    "    each_f1_scores = each_f1(y_te, clf.predict(test_target) )\n",
    "#     print(\"{} => {}[{}]: val:{:.4g}, test:{:.4g} | each_f1:{}\".format(target, name, mk, val_score, test_score, each_f1_scores.tolist()))\n",
    "    S = clf.best_estimator_.steps[-1][1].idxs\n",
    "    Summaries = train_target.iloc[S].Tweet.values\n",
    "    Summaries1 = Summaries[:mk].tolist()\n",
    "    Summaries2 = Summaries[mk:].tolist()\n",
    "    summ_lengths = [np.mean(train_target.iloc[S].num_tokens.values[:mk]), np.mean(train_target.iloc[S].num_tokens.values[mk:])]\n",
    "    msg = \"{},{},{},{:.4g},{:.4g},{:.2g},{:.2g},{:.4g},{:.4g},{:.4g}\".format(\n",
    "        target, name, mk, val_score, test_score,\n",
    "        *summ_lengths,\n",
    "        *each_f1_scores,\n",
    "        np.mean(each_f1_scores)\n",
    "        # \" ###### \".join(Summaries1),\n",
    "        # \" ###### \".join(Summaries2)\n",
    "    )\n",
    "    print(clf.best_estimator_)\n",
    "    print(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_kernels as kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/semi_supervised/_label_propagation.py:403: RuntimeWarning: invalid value encountered in true_divide\n",
      "  affinity_matrix /= normalizer[:, np.newaxis]\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/semi_supervised/_label_propagation.py:281: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.label_distributions_ /= normalizer\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/semi_supervised/_label_propagation.py:292: ConvergenceWarning: max_iter=1000 was reached without convergence.\n",
      "  category=ConvergenceWarning\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Stance</th>\n",
       "      <th>AGAINST</th>\n",
       "      <th>FAVOR</th>\n",
       "      <th>NONE</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>390</td>\n",
       "      <td>98</td>\n",
       "      <td>167</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>393</td>\n",
       "      <td>118</td>\n",
       "      <td>178</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stance   AGAINST  FAVOR  NONE  All\n",
       "predict                           \n",
       "0.0          390     98   167  655\n",
       "1.0            3     20    11   34\n",
       "All          393    118   178  689"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = TfidfVectorizer(\n",
    "            tokenizer = lambda x: x, ## we already have list of entities\n",
    "            lowercase = False,\n",
    "            binary = False,\n",
    "            use_idf = True,\n",
    "            preprocessor = None,\n",
    "            norm = None,\n",
    "            min_df = 2,\n",
    "        )\n",
    "X = bow.fit_transform(subset_train[\"tokens\"])\n",
    "model = LabelPropagation(kernel = lambda X, Y: kernels(X, Y, metric = \"cosine\"))\n",
    "model.fit(X.todense(), subset_train[\"labels\"].values)\n",
    "subset_train[\"predict\"] = model.transduction_\n",
    "pd.crosstab(index=subset_train['predict'], columns=subset_train['Stance'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Stance</th>\n",
       "      <th>AGAINST</th>\n",
       "      <th>FAVOR</th>\n",
       "      <th>NONE</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>160</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>160</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stance   AGAINST  FAVOR  NONE  All\n",
       "predict                           \n",
       "0.0          160     32    28  220\n",
       "All          160     32    28  220"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_test1 = test.query('Target==\"Atheism\"')\n",
    "X_test = bow.transform(subset_test1[\"tokens\"])\n",
    "subset_test1['predict'] = model.predict(X_test)\n",
    "pd.crosstab(index=subset_test1['predict'], columns=subset_test1['Stance'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Stance</th>\n",
       "      <th>AGAINST</th>\n",
       "      <th>FAVOR</th>\n",
       "      <th>NONE</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>238</td>\n",
       "      <td>70</td>\n",
       "      <td>113</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>304</td>\n",
       "      <td>92</td>\n",
       "      <td>117</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stance  AGAINST  FAVOR  NONE  All\n",
       "labels                           \n",
       "-1.0        238     70   113  421\n",
       "0.0          60      2     1   63\n",
       "1.0           6     20     3   29\n",
       "All         304     92   117  513"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target, pos, neg = \"Hillary Clinton\", [\"#lovewins\", \"@barackobama\", \"women\", \"proud\"], [\"#wakeupamerica\", \"email\", \"#benghazi\"]\n",
    "target, pos, neg = \"Atheism\", [\"#freethinker\", \"#freedom\", \"religi\"], [\"jesu\", \"lord\", \"bless\", \"#teamjesus\"]\n",
    "subset_train = assign(target, pos, neg)\n",
    "pd.crosstab(index=subset_train['labels'], columns=subset_train['Stance'], margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 62, 1: 26}) Counter({-1: 160, 1: 32})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:21:56 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=1.0,lambda=0.4,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-diff', MMDGreedy(C=1.0, lambdaa=0.4, mk=5))])\n",
      "Atheism,tok_idf-bow_exp_greedy-diff,5,0.9181,0.6769,7.6,9.2,0.8738,0.48,0.6769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:21:57 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=0.1,lambda=0.2,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-div',\n",
      "                 MMDGreedy(C=0.1, diff='div', lambdaa=0.2, mk=5))])\n",
      "Atheism,tok_idf-bow_exp_greedy-div,5,0.9194,0.6563,8.8,9.4,0.8407,0.4719,0.6563\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:21:58 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=0.1,lambda=0.2,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-diff', MMDGreedy(C=0.1, lambdaa=0.2, mk=5))])\n",
      "Atheism,tok_bow_exp_greedy-diff,5,0.9594,0.6769,7.6,10,0.8738,0.48,0.6769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:21:59 INFO [MMDGreedy:275]=> fitted MMDGreedy(C=0.1,lambda=0.1,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:21:59 INFO [LengthComp:275]=> fitted LengthComp(C=1.0,gamma=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)),\n",
      "                ('greedy-div',\n",
      "                 MMDGreedy(C=0.1, diff='div', lambdaa=0.1, mk=5))])\n",
      "Atheism,tok_bow_exp_greedy-div,5,0.9473,0.68,8.8,10,0.86,0.5,0.68\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')),\n",
      "                ('length', LengthComp(C=1.0, mk=5))])\n",
      "Atheism,tok_idf-bow_length,5,0.7615,0.6018,15,12,0.8652,0.3385,0.6018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [PRComp:275]=> fitted PRComp(C=10.0,alpha=0.8,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('pr', PRComp(C=10.0, alpha=0.8, mk=5))])\n",
      "Atheism,tok_idf-bow_cos_pr,5,0.7778,0.2343,12,8,0.1705,0.2981,0.2343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [PRComp:275]=> fitted PRComp(C=1.0,alpha=0.9,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [NNComp:275]=> fitted NNComp(C=10.0,m=5)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [NNComp:275]=> fitted NNComp(C=0.1,m=5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('pr', PRComp(C=1.0, alpha=0.9, mk=5))])\n",
      "Atheism,tok_bow_cos_pr,5,0.9111,0.5036,12,8.2,0.6587,0.3485,0.5036\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('nn', NNComp(C=10.0, mk=5))])\n",
      "Atheism,tok_idf-bow_cos_nn,5,0.8852,0.6589,11,9.4,0.8543,0.4634,0.6589\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('nn', NNComp(C=0.1, mk=5))])\n",
      "Atheism,tok_bow_cos_nn,5,0.9356,0.6478,11,10,0.8276,0.4681,0.6478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n",
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:00 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n",
      "/Users/umanga/git_repos/stancesumm/commons/functions.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = positives / counts1\n",
      "/Users/umanga/git_repos/stancesumm/commons/functions.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = positives / counts1\n",
      "/Users/umanga/git_repos/stancesumm/commons/functions.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prec = positives / counts1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('cos', CosineK()), ('all', AllComp(C=0.1))])\n",
      "Atheism,tok_bow_cos_all,5,0.5,0.1658,11,9.8,0.04848,0.2831,0.1658\n",
      "\n",
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('cos', CosineK()),\n",
      "                ('all', AllComp(C=0.1))])\n",
      "Atheism,tok_idf-bow_cos_all,5,0.5,nan,11,9.8,nan,0.2857,nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:01 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)), ('bow', BoW()),\n",
      "                ('exp', ExpK(gamma=0.0625)), ('all', AllComp(C=0.1))])\n",
      "Atheism,tok_bow_exp_all,5,0.5,0.2238,11,9.8,0.1591,0.2885,0.2238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/installed/apps/conda/envs/stancesumm/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "Jul-22 07:22:01 INFO [AllComp:275]=> fitted AllComp(C=0.1,m=all)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tok', PandasSelector(tokens)),\n",
      "                ('idf-bow', BoW(weights='idf')), ('exp', ExpK(gamma=0.0625)),\n",
      "                ('all', AllComp(C=0.1))])\n",
      "Atheism,tok_idf-bow_exp_all,5,0.5,0.1497,11,9.8,0.01242,0.287,0.1497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_test = test.query('Target==@target')\n",
    "test_target = test.query('Target == @target & Stance == @lst').reset_index()\n",
    "train_target = subset_train.query('Target == @target & Stance == @lst & labels != -1').reset_index()\n",
    "\n",
    "y_tr = np.array( 2 * train_target.labels - 1, dtype = int)\n",
    "y_te = np.ones(len(test_target), dtype = np.int8)\n",
    "\n",
    "# y_tr[np.where(train_target.Stance.values == \"AGAINST\")] = -1\n",
    "y_te[np.where(test_target.Stance.values == \"AGAINST\")] = -1\n",
    "train_target[\"y\"] = y_tr\n",
    "test_target[\"y\"] = y_te\n",
    "    \n",
    "print(Counter(y_tr), Counter(y_te))\n",
    "\n",
    "\n",
    "for steps, mk in product(STEPS, mks):\n",
    "    pipe, _hyperparams = get_model(steps, mk = mk)\n",
    "    name = \"_\".join(steps)\n",
    "    hyperparams = {}\n",
    "    for hp in _hyperparams:\n",
    "        hp_name, step = hp\n",
    "        hyperparams[\"{}__{}\".format(step, hp_name)] = HYPERPARAMS_GRID[hp_name]\n",
    "        # if step == \"greedy-div\" and hp_name == \"lambdaa\":\n",
    "        #     hyperparams[\"{}__{}\".format(step, hp_name)] = HYPERPARAMS_GRID[\"lambda2\"]\n",
    "#     print(\"{}, m={}, params={}\".format(name, mk, \";\".join(hyperparams.keys()) ))\n",
    "\n",
    "    clf = GridSearchCV(Pipeline(pipe), hyperparams, \n",
    "                cv = CV, n_jobs = N_JOBS, pre_dispatch = N_JOBS, verbose = 0, scoring = scorer, \n",
    "                return_train_score = False, error_score = 0.5)\n",
    "    clf.fit(train_target, train_target.y.values)\n",
    "    train_score, test_score = clf.score(train_target, train_target.y.values), clf.score(test_target, test_target.y.values)\n",
    "    val_score = clf.best_score_\n",
    "    each_f1_scores = each_f1(y_te, clf.predict(test_target) )\n",
    "#     print(\"{} => {}[{}]: val:{:.4g}, test:{:.4g} | each_f1:{}\".format(target, name, mk, val_score, test_score, each_f1_scores.tolist()))\n",
    "    S = clf.best_estimator_.steps[-1][1].idxs\n",
    "    Summaries = train_target.iloc[S].Tweet.values\n",
    "    Summaries1 = Summaries[:mk].tolist()\n",
    "    Summaries2 = Summaries[mk:].tolist()\n",
    "    summ_lengths = [np.mean(train_target.iloc[S].num_tokens.values[:mk]), np.mean(train_target.iloc[S].num_tokens.values[mk:])]\n",
    "    msg = \"{},{},{},{:.4g},{:.4g},{:.2g},{:.2g},{:.4g},{:.4g},{:.4g}\".format(\n",
    "        target, name, mk, val_score, test_score,\n",
    "        *summ_lengths,\n",
    "        *each_f1_scores,\n",
    "        np.mean(each_f1_scores)\n",
    "        # \" ###### \".join(Summaries1),\n",
    "        # \" ###### \".join(Summaries2)\n",
    "    )\n",
    "    print(clf.best_estimator_)\n",
    "    print(msg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
